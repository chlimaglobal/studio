
'use server';

/**
 * @fileOverview L√∫mina ‚Äî fluxo de chat para casais.
 */

import { ai } from '@/ai/genkit';
import { z } from 'zod';
import type { LuminaCoupleChatInput, LuminaChatOutput } from '@/lib/types';
import { LuminaCoupleChatInputSchema, LuminaChatOutputSchema } from '@/lib/types';
import { LUMINA_BASE_PROMPT } from '@/ai/lumina/prompt/luminaBasePrompt';


// === Fun√ß√£o externa chamada pela aplica√ß√£o ===
export async function generateCoupleSuggestion(input: LuminaCoupleChatInput): Promise<LuminaChatOutput> {
  return luminaCoupleChatFlow(input);
}

const luminaCoupleChatFlow = ai.defineFlow(
  {
    name: 'luminaCoupleChatFlow',
    inputSchema: LuminaCoupleChatInputSchema,
    outputSchema: LuminaChatOutputSchema,
     retrier: {
      maxAttempts: 3,
      backoff: {
        delayMs: 1500,
        multiplier: 2,
      },
    },
  },
  async (input) => {
    
    const mappedChatHistory = (input.chatHistory || []).map((msg) => ({
      role: msg.role === 'lumina' ? 'model' : ('user' as 'user' | 'model'),
      content: [
        {
          text: (msg.text || '').toString(),
        },
      ],
    }));

    const transactionsForContext = (input.allTransactions || []).slice(0, 50);

    const promptContext = [
        LUMINA_BASE_PROMPT,
        '',
        '### CONTEXTO SISTEMA (n√£o repita literalmente ao usu√°rio):',
        '- MODO CASAL ATIVADO',
        `- Usu√°rio Atual: ${input.user.displayName} (ID: ${input.user.uid})`,
        `- Parceiro(a): ${input.partner.displayName} (ID: ${input.partner.uid})`,
        `- Transa√ß√µes do casal (√∫ltimas ${transactionsForContext.length}):`,
        JSON.stringify(transactionsForContext, null, 2),
        input.audioText ? `- √Åudio transcrito: ${input.audioText}` : '- √Åudio transcrito: N/A',
        '',
        '### NOVA MENSAGEM DO USU√ÅRIO:',
        input.userQuery || '(mensagem vazia)',
        '',
        'Responda como L√∫mina, dirigindo-se ao casal de forma inclusiva, humana e proativa. Sempre termine com uma pergunta para engajar a conversa.',
    ].join('\n');

    try {
        const { output } = await ai.generate({
            model: 'googleai/gemini-1.5-flash',
            prompt: promptContext,
            history: mappedChatHistory,
            output: {
                schema: LuminaChatOutputSchema,
            },
        });

      if (!output || !output.text) {
        throw new Error("A L√∫mina n√£o retornou uma resposta v√°lida para o casal.");
      }
      
      return output;

    } catch (err) {
      console.error("üî• ERRO AO CHAMAR GEMINI (COUPLE):", err);
      return {
        text: "Tivemos uma pequena instabilidade, mas j√° estou de volta para ajudar voc√™s. Qual o pr√≥ximo passo?",
        suggestions: [
          "Resumo das despesas do casal",
          "Quem gastou mais esse m√™s?",
          "Criar uma meta financeira juntos"
        ]
      };
    }
  }
);
